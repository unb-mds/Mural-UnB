# üå≤ Modelos de Classifica√ß√£o no Scikit-Learn

O **Scikit-Learn** oferece uma ampla variedade de algoritmos de classifica√ß√£o prontos para uso, permitindo treinar e avaliar modelos de forma r√°pida e eficiente.  
Abaixo, exploramos os principais tipos de classificadores dispon√≠veis:

---

## üîπ √Årvores de Decis√£o (Decision Trees)

### O que s√£o?
√Årvores de decis√£o s√£o modelos baseados em regras hier√°rquicas que dividem os dados em regi√µes homog√™neas de acordo com os atributos.  

#### Vantagens
- F√°cil interpreta√ß√£o e visualiza√ß√£o.  
- N√£o requer normaliza√ß√£o dos dados.  
- Pode lidar com vari√°veis num√©ricas e categ√≥ricas.  

####  Exemplo Visual
<p> <img src="./assets/simple_didatic_decision_tree.png"> </p>
<p> <img src="./assets/tree_classification_exemple.gif"> </p>

### Exemplo com Scikit-Learn
```python
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(max_depth=3, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```
### üîπ Florestas Aleat√≥rias (Random Forest)
### O que s√£o?
Conjunto de v√°rias √°rvores de decis√£o treinadas em subconjuntos dos dados, cujas previs√µes s√£o combinadas.
<p> <img src="./assets/exemple_on_random_forest.png"> </p>
#### Vantagens
- Reduz o risco de overfitting.

- Melhor desempenho que √°rvores √∫nicas.

```python
Copy code
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
```
## üîπ Gradient Boosting (XGBoost, LightGBM, etc.)
O que √©?
M√©todo baseado em boosting, onde modelos fracos (√°rvores pequenas) s√£o treinados sequencialmente, cada um corrigindo os erros do anterior.

<p> <img src="./assets/exemple_on_XGBoost.png"> </p>

#### Vantagens
- Excelente desempenho em competi√ß√µes de ML.

- Muito eficaz em dados tabulares.

Exemplo com Scikit-Learn
```python
from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier()
model.fit(X_train, y_train)
```

## üîπ Support Vector Machines (SVM)
O que s√£o?
Algoritmos que buscam um hiperplano √≥timo para separar classes no espa√ßo de atributos.

#### Vantagens
- Eficaz em espa√ßos de alta dimens√£o.

- Funciona bem com margens claras entre classes.

#### Quando utilizar?
Dados com classes bem separadas.

Problemas de alta dimensionalidade.

```python
from sklearn.svm import SVC

model = SVC(kernel="rbf", C=1, gamma="scale")
model.fit(X_train, y_train)
```

## üîπ K-Nearest Neighbors (KNN)
### O que √©?
Classificador baseado em dist√¢ncia: um novo ponto √© classificado de acordo com a maioria dos r√≥tulos dos seus vizinhos mais pr√≥ximos.

<p> <img src="./assets/example_on_KNN_classifier.png"> </p>

#### Vantagens
- Simples e intuitivo.

- Funciona bem com poucos atributos.

> ‚ö†Ô∏è Importante: KNN √© sens√≠vel √† escala dos dados ‚Üí usar Feature Scaling.

```python
Copy code
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
```
## üîπ Regress√£o Log√≠stica
Apesar do nome, √© um modelo de classifica√ß√£o linear tambem.
Ele estima a probabilidade de uma amostra pertencer a uma classe utilizando a fun√ß√£o sigmoide.

<p> <img src="./assets/exemple_on_logistic_regression.png"> </p>

#### Vantagens
- Simples, r√°pido e interpret√°vel.

- √ötil como baseline.

```python
Copy code
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
```

üìä Compara√ß√£o entre Modelos de Classifica√ß√£o

| Modelo                  | Vantagens                                       | Desvantagens                                  | Quando Usar                                       |
| ----------------------- | ----------------------------------------------- | --------------------------------------------- | ------------------------------------------------- |
| **√Årvore de Decis√£o**   | F√°cil interpreta√ß√£o, lida com dados categ√≥ricos | Pode sofrer overfitting                       | Quando interpretabilidade √© importante            |
| **Random Forest**       | Reduz overfitting, robusto                      | Mais lento que √°rvore √∫nica                   | Dados tabulares gerais                            |
| **SVM**                 | Bom em alta dimens√£o, margens claras            | Lento em datasets muito grandes, exige tuning | Classes bem separadas, alta dimens√£o              |
| **KNN**                 | Simples, n√£o precisa de treino intensivo        | Sens√≠vel √† escala e a ru√≠do                   | Poucos atributos, dataset pequeno                 |
| **Regress√£o Log√≠stica** | R√°pida, interpret√°vel                           | Limitada a separa√ß√£o linear                   | Como baseline para compara√ß√£o                     |
| **Gradient Boosting**   | Alta performance, √≥timo em dados tabulares      | Mais complexo, tuning necess√°rio              | Competi√ß√£o de ML, quando performance √© prioridade |
